{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a96655",
   "metadata": {},
   "source": [
    "# Lead Conversion Prediction for Auxia\n",
    "\n",
    "Auxia, a rapidly growing company, aims to optimize its sales process and enhance customer retention through data-driven insights. In a competitive market, it is crucial to efficiently allocate sales resources and proactively engage with customers.\n",
    "\n",
    "**Your task is to develop a machine learning model that predicts the likelihood of a lead converting (or a customer churning) based on historical customer data.** This model will serve as a strategic tool for our sales team, enabling them to prioritize their efforts, improve efficiency, and implement targeted engagement strategies to maximize conversion rates and minimize churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c36082",
   "metadata": {},
   "source": [
    "# Phase 1: Data Exploration and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa7954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Data Exploration and Cleaning\n",
    "# This initial phase aims to assess the candidate's ability to understand raw data,\n",
    "# identify quality issues, and perform initial preparation steps efficiently.\n",
    "# It evaluates their foundational data handling skills, which are critical for any data science project.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Data Loading & Initial Inspection ---\n",
    "# Task: Load the dataset into a Pandas DataFrame.\n",
    "# Display the first few rows, check the data types of each column,\n",
    "# and obtain a summary of descriptive statistics.\n",
    "try:\n",
    "    df = pd.read_csv('data/dataset-big.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'data/dataset.csv' not found. Please ensure the file is in the correct path.\")\n",
    "    # In a real scenario, you might want to raise an exception or exit.\n",
    "    # For this exercise, we'll proceed with an empty DataFrame or similar handling.\n",
    "    df = pd.DataFrame() # Create an empty DataFrame to avoid errors in subsequent steps\n",
    "    print(\"Creating an empty DataFrame. Please ensure 'data/dataset.csv' exists and is accessible.\")\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"\\n--- Initial Data Inspection ---\")\n",
    "    # Display the first few rows of the dataset.\n",
    "    print(\"First 5 rows of the dataset:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Check the data types of each column and obtain non-null counts.\n",
    "    print(\"\\nInformation about the dataset (data types and non-null counts):\")\n",
    "    df.info()\n",
    "\n",
    "    # Obtain a summary of descriptive statistics for numerical columns.\n",
    "    print(\"\\nDescriptive statistics of the dataset:\")\n",
    "    print(df.describe())\n",
    "\n",
    "    # --- Missing Value Identification & Handling ---\n",
    "    # Task: Identify columns with missing values and quantify their extent.\n",
    "    # Propose and implement a strategy to handle these missing values,\n",
    "    # justifying your approach given the problem context.\n",
    "    print(\"\\n--- Missing Value Handling ---\")\n",
    "    # Identify columns with missing values and quantify their extent.\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"Missing values before handling:\\n\", missing_values[missing_values > 0])\n",
    "\n",
    "    # Propose and implement a strategy to handle these missing values.\n",
    "        # Justification for imputation over dropping: For a larger dataset like 'dataset-big.csv',\n",
    "        # dropping rows with missing values can lead to significant data loss, which is undesirable.\n",
    "        # Imputation allows us to retain more data points, preserving valuable information.\n",
    "        # Strategy:\n",
    "        # 1. Numerical columns: Use Median Imputation. Median is more robust to outliers\n",
    "        #    compared to mean and works well for skewed distributions.\n",
    "        # 2. Categorical columns: Use Mode Imputation. Fills with the most frequent category.\n",
    "\n",
    "    numerical_cols_with_missing = df.select_dtypes(include=['int64', 'float64']).columns[df.select_dtypes(include=['int64', 'float64']).isnull().any()].tolist()\n",
    "    categorical_cols_with_missing = df.select_dtypes(include=['object']).columns[df.select_dtypes(include=['object']).isnull().any()].tolist()\n",
    "\n",
    "    if numerical_cols_with_missing:\n",
    "        print(f\"\\nImputing numerical columns (median): {numerical_cols_with_missing}\")\n",
    "        for col in numerical_cols_with_missing:\n",
    "            median_val = df[col].median()\n",
    "            df[col].fillna(median_val, inplace=True)\n",
    "            print(f\"  - Filled missing values in '{col}' with median: {median_val}\")\n",
    "\n",
    "    if categorical_cols_with_missing:\n",
    "        print(f\"\\nImputing categorical columns (mode): {categorical_cols_with_missing}\")\n",
    "        for col in categorical_cols_with_missing:\n",
    "            mode_val = df[col].mode()[0] # .mode() can return multiple values if ties, [0] picks the first\n",
    "            df[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"  - Filled missing values in '{col}' with mode: {mode_val}\")\n",
    "\n",
    "    if not numerical_cols_with_missing and not categorical_cols_with_missing:\n",
    "        print(\"\\nNo missing values found to impute.\")\n",
    "\n",
    "    print(\"\\nMissing values after imputation:\\n\", df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "    # Propose and implement a strategy to handle these missing values.\n",
    "    # For 'data_usage_gb', which has one missing value in this sample dataset,\n",
    "    # a simple strategy is to drop the rows with missing values.\n",
    "    # Justification: Given the very small dataset size (10 rows), dropping one row will not significantly\n",
    "    # impact the data volume. For larger datasets, or if the missingness was not random,\n",
    "    # imputation (e.g., mean, median, mode for numerical, or mode for categorical) would be preferred to preserve data.\n",
    "    # initial_rows = len(df)\n",
    "    # df.dropna(inplace=True)\n",
    "    # rows_after_dropna = len(df)\n",
    "    # print(f\"\\nDropped {initial_rows - rows_after_dropna} rows with missing values.\")\n",
    "    # print(\"Missing values after dropping rows with any nulls:\\n\", df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "    # --- Duplicate Records Handling ---\n",
    "    # Task: Check and handle duplicate records.\n",
    "    print(\"\\n--- Duplicate Records Handling ---\")\n",
    "    # Check for duplicate values.\n",
    "    num_duplicates = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows found: {num_duplicates}\")\n",
    "\n",
    "    # Drop duplicate values if any.\n",
    "    # Justification: Assuming duplicate rows are data entry errors and not valid repeated observations.\n",
    "    if num_duplicates > 0:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        print(f\"Duplicates dropped. New number of rows: {len(df)}\")\n",
    "    else:\n",
    "        print(\"No duplicate rows to drop.\")\n",
    "\n",
    "    # --- Outlier Detection (Conceptual/Quick Implementation) ---\n",
    "    # Task: Briefly discuss how you would identify potential outliers in numerical features.\n",
    "    # If time permits, demonstrate a quick method to visualize or flag them.\n",
    "    print(\"\\n--- Outlier Detection ---\")\n",
    "    print(\"Discussion: Outliers are data points significantly different from others. They can \"\n",
    "          \"impact model performance. Common identification methods include:\\n\"\n",
    "          \"1. Visualization: Box plots (good for visualizing distribution and potential outliers),\\n\"\n",
    "          \"   histograms, and scatter plots (for relationships between variables).\\n\"\n",
    "          \"2. Statistical methods: Z-score (for normally distributed data) or IQR (Interquartile Range) method.\\n\"\n",
    "          \"   - IQR Method: Data points falling below Q1 - 1.5*IQR or above Q3 + 1.5*IQR are flagged.\\n\"\n",
    "          \"   - Z-score Method: Data points with a Z-score (deviation from mean in std dev units) \"\n",
    "          \"     greater than a certain threshold (e.g., 2 or 3) are considered outliers.\\n\"\n",
    "          \"Handling strategies include trimming (removal), Winsorization (capping), or transformation.\\n\"\n",
    "          \"The choice depends on whether outliers are errors or genuine extreme values, and their impact on model goals.\")\n",
    "\n",
    "    # Quick visualization example for numerical features:\n",
    "    # Identifying numerical columns for outlier visualization (excluding 'customer_id' as it's an identifier and 'churn' as it's a target)\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    if 'customer_id' in numerical_cols:\n",
    "        numerical_cols.remove('customer_id')\n",
    "    # if 'churn' in numerical_cols: # 'churn' is often a target variable and not typically analyzed for outliers in this context\n",
    "        # numerical_cols.remove('churn')\n",
    "\n",
    "    if numerical_cols: # Only proceed if there are numerical columns to plot\n",
    "        print(\"\\nVisualizing numerical features for potential outliers (Histograms and Box plots):\")\n",
    "        for col in numerical_cols:\n",
    "            plt.figure(figsize=(14, 6))\n",
    "\n",
    "            # Histogram\n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.histplot(df[col], kde=True)\n",
    "            plt.title(f'Distribution of {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequency')\n",
    "\n",
    "            # Box plot\n",
    "            plt.subplot(1, 2, 2)\n",
    "            sns.boxplot(y=df[col])\n",
    "            plt.title(f'Box Plot of {col}')\n",
    "            plt.ylabel(col)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"\\nNo suitable numerical columns found for outlier visualization.\")\n",
    "\n",
    "    # --- Basic Data Transformation/Cleaning (Date Columns - Conceptual) ---\n",
    "    # Task: Ensure date columns (signup_date, last_activity_date) are in the correct format.\n",
    "    # Create a new numerical feature: lead_age_days from signup_date.\n",
    "    print(\"\\n--- Date Column Transformation (Conceptual) ---\")\n",
    "    print(\"This dataset does not contain explicit date columns like 'signup_date' or 'last_activity_date'.\\n\"\n",
    "          \"If it did, the process would involve:\\n\"\n",
    "          \"1. Converting to datetime format: `df['signup_date'] = pd.to_datetime(df['signup_date'])`\\n\"\n",
    "          \"   `df['last_activity_date'] = pd.to_datetime(df['last_activity_date'])`\\n\"\n",
    "          \"2. Creating new features, e.g., 'lead_age_days':\\n\"\n",
    "          \"   `from datetime import datetime`\\n\"\n",
    "          \"   `df['lead_age_days'] = (datetime.now() - df['signup_date']).dt.days`\\n\"\n",
    "          \"This evaluates basic date manipulation skills and early-stage feature creation, crucial for time-series analysis.\")\n",
    "\n",
    "    # --- Final Data State Summary ---\n",
    "    print(\"\\n--- Data Cleaning Summary ---\")\n",
    "    print(\"Data cleaning and initial exploration completed. The dataset now has:\")\n",
    "    print(f\"- {len(df)} rows\")\n",
    "    print(f\"- {df.shape[1]} columns\")\n",
    "    print(\"Review df.info() and df.describe() for the updated state of the DataFrame.\")\n",
    "\n",
    "    # Display the cleaned data's head to confirm changes and new structure\n",
    "    print(\"\\nCleaned DataFrame Head:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"\\nDataFrame is empty. Cannot perform further data exploration and cleaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc9d5f",
   "metadata": {},
   "source": [
    "# Phase 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Feature Engineering\n",
    "# This phase focuses on transforming raw data into features to enhance machine learning model performance.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'df' DataFrame is available from Phase 1 (already loaded, missing values & duplicates handled).\n",
    "# If running this cell independently and 'df' is not defined, uncomment and run the basic loading/cleaning:\n",
    "# try:\n",
    "#     df = pd.read_csv('data/dataset.csv')\n",
    "#     df.dropna(inplace=True) # Basic cleaning for standalone run\n",
    "#     df.drop_duplicates(inplace=True)\n",
    "#     print(\"Dataset loaded and basic cleaning performed for standalone execution.\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"Error: 'data/dataset.csv' not found. Cannot proceed with Feature Engineering without data.\")\n",
    "#     df = pd.DataFrame() # Ensure df is an empty DataFrame to prevent further errors\n",
    "#     exit()\n",
    "\n",
    "if df.empty:\n",
    "    print(\"DataFrame is empty. Cannot perform Feature Engineering.\")\n",
    "else:\n",
    "    print(\"\\n--- Feature Engineering ---\\n\")\n",
    "\n",
    "    # --- Deriving New Features ---\n",
    "    # Task: Create at least two new meaningful features that you believe would be predictive of lead conversion.\n",
    "    # Explain your rationale for each.\n",
    "    print(\"Deriving New Features:\\n\")\n",
    "\n",
    "    # Feature 1: 'monthly_bill_per_gb'\n",
    "    # Rationale: This feature captures the cost-efficiency of a customer's plan relative to their data usage.\n",
    "    # A higher value might indicate less value for money, potentially leading to higher churn.\n",
    "    # We add a small epsilon (1e-6) to 'data_usage_gb' to avoid division by zero.\n",
    "    # Based on our Phase 1 cleaning, 'data_usage_gb' has a minimum of 8.0, so division by zero is not an issue with this dataset.\n",
    "    df['monthly_bill_per_gb'] = df['monthly_bill'] / (df['data_usage_gb'] + 1e-6)\n",
    "    print(\" - Created 'monthly_bill_per_gb' (monthly_bill / data_usage_gb).\")\n",
    "\n",
    "    # Feature 2: 'has_support_tickets'\n",
    "    # Rationale: A binary indicator (0 or 1) that signals whether a customer has opened any support tickets.\n",
    "    # Customers who have experienced issues and needed support might have different churn patterns.\n",
    "    # This simplifies 'support_tickets_opened' into a direct indicator of interaction with support,\n",
    "    # which can be a strong signal for customer dissatisfaction or engagement.\n",
    "    df['has_support_tickets'] = (df['support_tickets_opened'] > 0).astype(int)\n",
    "    print(\" - Created 'has_support_tickets' (binary: 1 if support_tickets_opened > 0, else 0).\\n\")\n",
    "\n",
    "    # Display DataFrame head with new features\n",
    "    print(\"DataFrame after deriving new features (head):\")\n",
    "    print(df.head())\n",
    "    print(\"\\nNew DataFrame Info after feature derivation:\")\n",
    "    df.info()\n",
    "\n",
    "    # --- Handling Categorical Variables ---\n",
    "    # Task: Choose and implement an appropriate encoding strategy for these features.\n",
    "    # Justify your choice, considering potential issues like high cardinality or ordinality.\n",
    "    print(\"\\n\\n--- Handling Categorical Variables ---\\n\")\n",
    "\n",
    "    # Identify categorical columns (excluding 'customer_id' as it's an identifier and not for encoding)\n",
    "    categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "    if 'customer_id' in categorical_cols:\n",
    "        categorical_cols.remove('customer_id') # Remove identifier column\n",
    "\n",
    "    if not categorical_cols:\n",
    "        print(\"No categorical columns found for encoding (excluding 'customer_id').\")\n",
    "    else:\n",
    "        print(f\"Categorical columns to encode: {categorical_cols}\")\n",
    "\n",
    "        # Strategy: One-Hot Encoding for 'subscription_type'\n",
    "        # Justification: 'subscription_type' is a nominal (unordered) categorical feature and\n",
    "        # has low cardinality (e.g., 'Basic', 'Premium', 'Enterprise').\n",
    "        # One-hot encoding creates binary columns for each category, which is suitable for\n",
    "        # most ML algorithms and avoids implying any false ordinal relationship that\n",
    "        # numerical encoding might introduce. It's a robust choice for real-time inference\n",
    "        # systems due to its simplicity and interpretability.\n",
    "        print(\"\\nApplying One-Hot Encoding to categorical features.\\n\")\n",
    "        onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "        # Fit and transform the categorical data\n",
    "        encoded_features = onehot_encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "        # Create a DataFrame from the encoded features with proper column names\n",
    "        encoded_df = pd.DataFrame(encoded_features, columns=onehot_encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "        # Reset index of original df to ensure proper concatenation\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Concatenate the original DataFrame (dropping original categorical columns) with the new encoded DataFrame\n",
    "        df = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "        print(\"Categorical features encoded using One-Hot Encoding.\")\n",
    "\n",
    "    print(\"\\nDataFrame after encoding categorical features (head):\")\n",
    "    print(df.head())\n",
    "    print(\"\\nNew DataFrame Info after encoding:\")\n",
    "    df.info()\n",
    "\n",
    "\n",
    "    # --- Numerical Feature Scaling ---\n",
    "    # Task: Discuss whether numerical features like monthly_bill or data_usage_gb should be scaled\n",
    "    # before model training. If so, choose and apply a scaling method (e.g., Standardization or Normalization)\n",
    "    # and explain why.\n",
    "    print(\"\\n\\n--- Numerical Feature Scaling ---\\n\")\n",
    "    print(\"Discussion: Numerical features should often be scaled before model training, especially for algorithms \"\n",
    "          \"sensitive to feature magnitudes or distances (e.g., K-Nearest Neighbors, Support Vector Machines, \"\n",
    "          \"Neural Networks, and gradient descent-based algorithms). Scaling helps prevent features with larger \"\n",
    "          \"ranges from dominating the learning process and can lead to faster convergence. \\n\"\n",
    "          \"Choice of method: MinMaxScaler (Normalization) or StandardScaler (Standardization).\\n\"\n",
    "          \"  - MinMaxScaler scales features to a fixed range (usually 0 to 1). It's suitable when you need \"\n",
    "          \"    features to be within a specific bounded range.\\n\"\n",
    "          \"  - StandardScaler scales features to have a mean of 0 and a standard deviation of 1. It's preferred \"\n",
    "          \"    when data follows a Gaussian distribution or when algorithms assume zero mean and unit variance.\\n\"\n",
    "          \"Justification for MinMaxScaler: Given the mixed ranges and distributions of our numerical features \"\n",
    "          \"(e.g., 'monthly_bill' vs. 'customer_service_score'), MinMaxScaler will transform all numerical features \"\n",
    "          \"into a common scale [0, 1] without distorting the shape of the distribution or introducing outliers \"\n",
    "          \"sensitivity from standardization. This is generally suitable for models that are sensitive to the \"\n",
    "          \"absolute scale of the data and helps ensure equal contribution from all features.\")\n",
    "\n",
    "    # Identify numerical columns for scaling (excluding 'customer_id', the target 'churn', and binary features)\n",
    "    numerical_features_to_scale = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    if 'customer_id' in numerical_features_to_scale:\n",
    "        numerical_features_to_scale.remove('customer_id')\n",
    "    if 'churn' in numerical_features_to_scale: # 'churn' is our target variable, not a feature to scale\n",
    "        numerical_features_to_scale.remove('churn')\n",
    "    if 'has_support_tickets' in numerical_features_to_scale: # This is a binary feature (0 or 1), no need to scale\n",
    "        numerical_features_to_scale.remove('has_support_tickets')\n",
    "\n",
    "    # Also remove any one-hot encoded columns from this list as they are already binary\n",
    "    # We assume one-hot encoded columns are not in original numerical_cols, but a defensive check\n",
    "    # would involve checking `onehot_encoder.get_feature_names_out(categorical_cols)`\n",
    "\n",
    "    if not numerical_features_to_scale:\n",
    "        print(\"\\nNo numerical features found for scaling (excluding customer_id, churn, and binary features).\")\n",
    "    else:\n",
    "        print(f\"\\nNumerical features to scale: {numerical_features_to_scale}\\n\")\n",
    "        scaler = MinMaxScaler()\n",
    "        df[numerical_features_to_scale] = scaler.fit_transform(df[numerical_features_to_scale])\n",
    "        print(\"Numerical features scaled using MinMaxScaler.\")\n",
    "\n",
    "    print(\"\\nDataFrame after scaling numerical features (head):\")\n",
    "    print(df.head())\n",
    "    print(\"\\nNew DataFrame Info after scaling:\")\n",
    "    df.info()\n",
    "\n",
    "    # --- Final Feature Engineering Summary ---\n",
    "    print(\"\\n\\n--- Feature Engineering Summary ---\")\n",
    "    print(\"Feature engineering completed. The dataset is now transformed with new features and scaled data.\")\n",
    "    print(\"It is now ready for the model training phase.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83fc0c9",
   "metadata": {},
   "source": [
    "# Phase 3: ML Model Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: ML Model Application\n",
    "# This phase focuses on selecting an appropriate model, training it, and discussing hyperparameters.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE # New import for handling class imbalance\n",
    "\n",
    "# Assuming 'df' DataFrame is available from Phase 2 (features engineered and scaled).\n",
    "# If running this cell independently, ensure df is fully prepared as per Phase 1 and 2.\n",
    "\n",
    "if df.empty:\n",
    "    print(\"DataFrame is empty. Cannot perform ML Model Application.\")\n",
    "else:\n",
    "    print(\"\\n--- ML Model Application ---\\n\")\n",
    "\n",
    "    # --- Model Choice Justification ---\n",
    "    # Task: Given the problem of predicting lead conversion (a binary classification task, assumed 'churn' here),\n",
    "    # which machine learning model would you choose to start with, and why? Briefly explain its core principle.\n",
    "    print(\"Model Choice Justification:\")\n",
    "    print(\"For predicting customer churn (a binary classification task where 'churn' is 0 or 1), I would choose \"\n",
    "          \"Logistic Regression as the initial model.\\n\"\n",
    "          \"Core Principle: Logistic Regression is a linear model that uses a logistic (sigmoid) function to \"\n",
    "          \"model the probability of a binary outcome. It takes a linear combination of input features and \"\n",
    "          \"transforms it into a probability between 0 and 1. If this probability is above a certain threshold \"\n",
    "          \"(e.g., 0.5), it classifies the instance as the positive class (churn), otherwise as the negative class.\\n\"\n",
    "          \"Why Logistic Regression:\\n\"\n",
    "          \"1.  Interpretability: It is highly interpretable. The coefficients can indicate the strength and \"\n",
    "          \"    direction of the relationship between each feature and the log-odds of churn, which is valuable \"\n",
    "          \"    for business stakeholders (e.g., sales team) to understand why a customer might churn.\\n\"\n",
    "          \"2.  Computational Efficiency: It is computationally efficient to train and predict, making it ideal \"\n",
    "          \"    for quick iterations and establishing a solid baseline within a time-constrained environment.\\n\"\n",
    "          \"3.  Baseline Model: It serves as an excellent baseline model against which more complex models can be compared. \"\n",
    "          \"    If Logistic Regression performs reasonably well, it might be sufficient, or it provides a benchmark \"\n",
    "          \"    to measure the improvements offered by more sophisticated algorithms.\\n\"\n",
    "          \"4.  Probabilistic Output: It directly provides probability scores, which are useful for ranking customers \"\n",
    "          \"    by their likelihood of churn and setting operational thresholds.\")\n",
    "\n",
    "    # --- Data Splitting & Model Training (with SMOTE for Imbalance Handling) ---\n",
    "    # Task: Split your prepared data into training and testing sets. Train your chosen model on the training data.\n",
    "    print(\"\\n\\n--- Data Splitting & Model Training (with SMOTE) ---\\n\")\n",
    "\n",
    "    # Define features (X) and target (y)\n",
    "    # 'churn' is our target variable\n",
    "    # 'customer_id' is an identifier and should not be used as a feature\n",
    "    # All other columns (including newly engineered and encoded ones) are features.\n",
    "    X = df.drop(columns=['customer_id', 'churn'], errors='ignore') # 'errors=ignore' handles cases where cols might already be dropped\n",
    "    y = df['churn']\n",
    "\n",
    "    print(f\"Features (X) shape: {X.shape}\")\n",
    "    print(f\"Target (y) shape: {y.shape}\")\n",
    "\n",
    "    # Split the data into training and testing sets (e.g., 80% train, 20% test)\n",
    "    # stratify=y is crucial here to maintain the original class proportion in train/test splits\n",
    "    # in both the training and testing sets before any resampling.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    print(f\"\\nOriginal Training set shape (X_train, y_train): {X_train.shape}, {y_train.shape}\")\n",
    "    print(f\"Original Testing set shape (X_test, y_test): {X_test.shape}, {y_test.shape}\")\n",
    "    print(f\"Original Training set class distribution (y_train):\\n{y_train.value_counts()}\")\n",
    "\n",
    "\n",
    "    print(\"\\nApplying SMOTE to the training data to handle class imbalance...\")\n",
    "    # Initialize SMOTE - default sampling_strategy='auto' balances all classes\n",
    "    # random_state for reproducibility\n",
    "    smote = SMOTE(random_state=42)\n",
    "\n",
    "    # Apply SMOTE only on the training data to avoid data leakage\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    print(f\"Resampled Training set shape (X_train_resampled, y_train_resampled): {X_train_resampled.shape}, {y_train_resampled.shape}\")\n",
    "    print(f\"Class distribution after SMOTE (y_train_resampled):\\n{y_train_resampled.value_counts()}\")\n",
    "\n",
    "\n",
    "    # Initialize the Logistic Regression model\n",
    "    # 'lbfgs' is a good default solver. Increased max_iter for convergence with resampled data.\n",
    "    # C=0.5 (from your previous code) is a good starting point for regularization.\n",
    "    model = LogisticRegression(random_state=42, solver='lbfgs', C=0.5, max_iter=1000)\n",
    "\n",
    "    # Train the model on the RESAMPLED training data\n",
    "    print(\"\\nTraining Logistic Regression model on resampled data...\")\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    # Make predictions on the ORIGINAL (unresampled) test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] # Probabilities for the positive class\n",
    "\n",
    "    # --- Model Evaluation (basic metrics) ---\n",
    "    # These will be more thoroughly evaluated in Phase 4.\n",
    "    print(\"\\n--- Initial Model Evaluation (on Test Set) ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"\\nClassification Report (initial):\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix (initial):\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "    # --- Brief Hyperparameter Discussion ---\n",
    "    # Task: Identify one or two key hyperparameters for your chosen model.\n",
    "    # How might adjusting these impact model performance or prevent overfitting?\n",
    "    print(\"\\n\\n--- Brief Hyperparameter Discussion (Logistic Regression) ---\\n\")\n",
    "    print(\"Key Hyperparameters for Logistic Regression:\\n\"\n",
    "          \"1.  C (Inverse of regularization strength): \\n\"\n",
    "          \"    - Role: `C` controls the strength of regularization. Regularization is a technique used to prevent overfitting \"\n",
    "          \"      by penalizing large coefficients. A smaller `C` value specifies stronger regularization (more penalty).\\n\"\n",
    "          \"    - Impact on Performance: \\n\"\n",
    "          \"      - Small `C` (strong regularization): Can lead to simpler models, reduce overfitting, and improve generalization \"\n",
    "          \"        on unseen data. However, if too small, it might lead to underfitting (model too simple).\\n\"\n",
    "          \"      - Large `C` (weak regularization): Allows the model to fit the training data more closely, potentially leading \"\n",
    "          \"        to overfitting, especially with noisy or complex data.\\n\"\n",
    "          \"    - Prevention of Overfitting: Decreasing `C` (increasing regularization) helps prevent overfitting by forcing \"\n",
    "          \"      the model to be less complex and generalize better.\\n\"\n",
    "          \"\\n\"\n",
    "          \"2.  solver: \\n\"\n",
    "          \"    - Role: The algorithm to use in the optimization problem. Different solvers work best with different types of data \"\n",
    "          \"      and regularization penalties (L1/L2).\\n\"\n",
    "          \"    - Examples: \\'liblinear\\', \\'lbfgs\\', \\'sag\\', \\'saga\\', \\'newton-cg\\'.\\n\"\n",
    "          \"    - Impact on Performance: The choice of solver can impact convergence speed and whether the model finds the global \"\n",
    "          \"      optimum. \\'liblinear\\' is often good for small datasets and supports both L1 and L2 regularization. \\'lbfgs\\' is a \"\n",
    "          \"      good default for most problems, especially larger ones.\\n\"\n",
    "          \"    - Prevention of Overfitting: Indirectly, by enabling efficient convergence to a stable solution, the solver contributes \"\n",
    "          \"      to finding the optimal model parameters which can support regularization in preventing overfitting.\\n\")\n",
    "\n",
    "    print(\"\\n--- ML Model Application Summary ---\\n\")\n",
    "    print(\"Logistic Regression model chosen, trained, and evaluated. Initial performance metrics \"\n",
    "          \"and hyperparameter considerations discussed. The model is now ready for further tuning \"\n",
    "          \"and deeper analysis.\")\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f31cc",
   "metadata": {},
   "source": [
    "# Phase 4: Model Performance Evaluation and Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf60b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: Model Performance Evaluation and Explanation\n",
    "# This phase assesses the model's performance, interprets results, and focuses on communication to stakeholders.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "\n",
    "# Assuming 'X_test', 'y_test', 'y_pred', and 'y_pred_proba' (from Phase 3) are available.\n",
    "# If running this cell independently and these are not defined, you would need to re-run Phase 1, 2, and 3 first.\n",
    "\n",
    "if 'y_test' not in locals() or 'y_pred' not in locals() or 'y_pred_proba' not in locals():\n",
    "    print(\"Required variables (y_test, y_pred, y_pred_proba) from Phase 3 are not available. Please run Phase 3 first.\")\n",
    "else:\n",
    "    print(\"\\n--- Model Performance Evaluation and Explanation ---\\n\")\n",
    "\n",
    "    # --- Metric Selection & Calculation ---\n",
    "    # Task: Given that the business goal is to identify high-potential leads for the sales team,\n",
    "    # which evaluation metric(s) would be most appropriate for your model?\n",
    "    # Calculate and report these metrics on your test set.\n",
    "    print(\"Metric Selection & Calculation:\\n\")\n",
    "    print(\"For identifying high-potential leads (positive class, i.e., likely to convert/not churn, or 'churn' as 0), \"\n",
    "          \"and considering the potential for imbalanced datasets (e.g., fewer churners than non-churners), \"\n",
    "          \"accuracy alone can be misleading. Therefore, the following metrics are most appropriate:\\n\"\n",
    "          \"1.  Precision: Important when False Positives (predicting a lead will convert but they don't) are costly. \"\n",
    "          \"    Wasting sales team's time on non-converting leads can be inefficient.\\n\"\n",
    "          \"2.  Recall (Sensitivity): Important when False Negatives (missing a high-potential lead who would have converted) \"\n",
    "          \"    are costly. Missing potential customers means lost opportunities for Auxia.\\n\"\n",
    "          \"3.  F1-Score: The harmonic mean of Precision and Recall. Provides a single score that balances both, \"\n",
    "          \"    useful when a balance between avoiding false positives and false negatives is desired.\\n\"\n",
    "          \"4.  AUC-ROC (Area Under the Receiver Operating Characteristic Curve): Measures the classifier's ability \"\n",
    "          \"    to distinguish between classes across various classification thresholds. It's robust to class imbalance \"\n",
    "          \"    and good for overall model comparison.\\n\")\n",
    "\n",
    "    # Calculate and report the chosen metrics\n",
    "    model_accuracy = accuracy_score(y_test, y_pred)\n",
    "    model_precision = precision_score(y_test, y_pred, zero_division=0) # zero_division=0 to handle no positive predictions gracefully\n",
    "    model_recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    model_f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    model_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    print(f\"Calculated Metrics on Test Set:\")\n",
    "    print(f\"  Accuracy: {model_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {model_precision:.4f}\")\n",
    "    print(f\"  Recall: {model_recall:.4f}\")\n",
    "    print(f\"  F1-Score: {model_f1:.4f}\")\n",
    "    print(f\"  AUC-ROC Score: {model_roc_auc:.4f}\\n\")\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Predicted 0 (No Churn)', 'Predicted 1 (Churn)'],\n",
    "                yticklabels=['Actual 0 (No Churn)', 'Actual 1 (Churn)'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {model_roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Interpreting Results & Diagnosing Issues ---\n",
    "    # Task: Based on your chosen metrics, how is your model performing?\n",
    "    # If the accuracy is hypothetically 90%, what other information would you need to interpret if this is 'good' or not?\n",
    "    # If your model is underperforming, what are two potential reasons, and how would you investigate them?\n",
    "    print(\"\\n\\n--- Interpreting Results & Diagnosing Issues ---\\n\")\n",
    "    print(\"Model Performance Interpretation (based on the sample data and metrics above):\\n\"\n",
    "          \"The model's performance on this very small test set (which will have only a few data points) \"\n",
    "          \"gives an initial indication. The Accuracy, Precision, Recall, and F1-Score provide a balanced \"\n",
    "          \"view of how well the model identifies both churners and non-churners. The AUC-ROC score gives an \"\n",
    "          \"overall measure of the model's ability to discriminate between the classes.\\n\"\n",
    "          \"\\n\"\n",
    "          \"Hypothetical Accuracy of 90%: Is it 'good'?\\n\"\n",
    "          \"If the accuracy were hypothetically 90%, to interpret if this is 'good' or not, I would need:\\n\"\n",
    "          \"1.  Baseline Accuracy: What is the accuracy if we simply predict the majority class? If 90% of customers \"\n",
    "          \"    do NOT churn, then a model that always predicts 'no churn' would already have 90% accuracy. \"\n",
    "          \"    In such a case, 90% accuracy for our model isn't impressive. We need to beat this baseline.\\n\"\n",
    "          \"2.  Class Imbalance: The distribution of churn vs. no-churn customers. If the dataset is highly imbalanced \"\n",
    "          \"    (e.g., 95% no-churn, 5% churn), high accuracy might just reflect predicting the majority class well, \"\n",
    "          \"    while failing to identify the minority class (churners), which is often the business focus.\\n\"\n",
    "          \"3.  Business Context & Cost of Errors: The specific costs associated with False Positives (misclassifying \"\n",
    "          \"    a non-churner as a churner, e.g., wasted sales efforts) and False Negatives (missing a churner, \"\n",
    "          \"    e.g., lost revenue). For Auxia, missing high-potential leads (False Negatives on conversion/churn as 0) \"\n",
    "          \"    is likely more costly than occasionally pursuing a lead that doesn't convert.\\n\"\n",
    "          \"4.  Other Metrics: Precision, Recall, F1-Score, and AUC-ROC are crucial. A high precision might mean \"\n",
    "          \"    the identified high-potential leads are genuinely good, while low recall means many true high-potential \"\n",
    "          \"    leads are missed. The F1-score balances this, and AUC-ROC provides an overall discriminatory power.\\n\"\n",
    "          \"\\n\"\n",
    "          \"If your model is underperforming, two potential reasons and how to investigate them:\\n\"\n",
    "          \"1.  Underfitting (High Bias): The model is too simple to capture the underlying patterns in the data.\\n\"\n",
    "          \"    - Investigation: Check if both training and test set performance are low. This indicates the model \"\n",
    "          \"      is not learning adequately. Potential solutions include: adding more relevant features (feature engineering),\"\n",
    "          \"      using a more complex model (e.g., Random Forest, Gradient Boosting), reducing regularization (for Logistic Regression, increase 'C').\\n\"\n",
    "          \"2.  Overfitting (High Variance): The model has learned the training data too well, including its noise, \"\n",
    "          \"    and performs poorly on unseen data.\\n\"\n",
    "          \"    - Investigation: Check if training performance is significantly higher than test performance. This is \"\n",
    "          \"      a classic sign of overfitting. Potential solutions include: getting more data, adding more regularization \"\n",
    "          \"      (for Logistic Regression, decrease 'C'), simplifying the model (e.g., reducing `max_depth` in trees), \"\n",
    "          \"      feature selection (removing noisy/irrelevant features), or using cross-validation more rigorously.\\n\")\n",
    "\n",
    "    # --- Explaining Findings to Non-Technical Stakeholders ---\n",
    "    # Task: Imagine you are presenting this model to Auxia's Head of Sales.\n",
    "    # How would you explain your model's performance and its potential impact on their lead qualification process,\n",
    "    # avoiding technical jargon?\n",
    "    print(\"\\n\\n--- Explaining Findings to Non-Technical Stakeholders (e.g., Auxia's Head of Sales) ---\\n\")\n",
    "    print(\"\\\"Good morning/afternoon, Head of Sales,\")\n",
    "    print(\"We've developed a new predictive model that can help your sales team focus on the right leads. \"\n",
    "          \"Think of it as a smart assistant that helps us understand which customers are most likely to stay with us.\")\n",
    "    print(\"\\nBased on our initial tests, this model is quite good at identifying potential churners before they leave.\"\n",
    "          \"Specifically, when the model flags a customer as high-risk, it's correct about X% of the time (referencing Precision).\"\n",
    "          \"And, out of all the customers who actually churn, our model is able to catch Y% of them (referencing Recall).\"\n",
    "          \"Our overall goal is to make sure we're not missing valuable customers while also making sure we don't \"\n",
    "          \"waste your team's time on leads that won't convert.\")\n",
    "    print(\"\\nThis model can significantly impact your lead qualification process by allowing your team to:\")\n",
    "    print(\"1.  Prioritize Leads: Focus valuable sales efforts on customers who are genuinely at risk of churning, \"\n",
    "          \"    allowing for timely intervention and targeted retention strategies.\")\n",
    "    print(\"2.  Increase Efficiency: Reduce the time spent on low-potential customers, reallocating resources to \"\n",
    "          \"    more promising interactions.\")\n",
    "    print(\"3.  Boost Retention: By proactively identifying and addressing customer needs, we can potentially \"\n",
    "          \"    improve overall customer satisfaction and retention rates, directly contributing to our revenue goals.\")\n",
    "    print(\"\\nWe're continuously refining this model, but this initial version provides a strong foundation \"\n",
    "          \"for a more data-driven approach to customer retention. We're excited to see how this can empower your team.\\\"\")\n",
    "\n",
    "    print(\"\\n\\n--- Phase 4 Summary ---\\n\")\n",
    "    print(\"Model evaluation complete, with metrics calculated, interpreted, and insights prepared for stakeholder communication.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
